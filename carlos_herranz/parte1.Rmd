---
title: "Práctica 9 - PLN"
author: 
"Carlos Herranz Bascuñan" (<carlos.herranzb@alumnos.upm.es>)
"Alberto Barreiro Tasende" (<alberto.barreiro@alumnos.upm.es>)
"Raquel Cassinello Montes" (<r.cmontes@alumnos.upm.es>)
date: "2026-01-12"
group: 9
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
```

## R Markdown

Parte 1 -  Descarga y extracción de enlaces a todos los artículos y su estructura

Esta función se encarga que dado la URL de la pagina del codigo penal leerla y devolver el texto de esta
```{r}
#Funcion para cargar la pagina web del codigo penal
cargar_texto<-function(){
  #URL de la pagina del codigo penal
  URL_codigo_penal<-"https://www.conceptosjuridicos.com/codigo-penal/"
  #Leemos la pagina del codigo penal
  codigo_penal_pagina <- paste(readLines(URL_codigo_penal,encoding = "UTF-8"),collapse="\n")
  return(codigo_penal_pagina)
}

```

Con la pagina ya cargada ahora tocara buscar los links en esta pagina para ello usamos esta función que con una pagina web dada devuelve todos los links de articulos de esta pagina web, ya sean bis o normal
```{r}
#Funcion para dada una pagina web devolver todos los links de los articulos de esta
buscar_links_articulos<-function(pagina_web_codigo_penal){
  #Buscamos en la pagina todos los links
  library(stringr)
  #patron_solo_articulo<-"https://www.conceptosjuridicos.com/codigo-penal-articulo-[1-90]*/"
  patron<-"https://www.conceptosjuridicos.com/codigo-penal-articulo-[1-90]*[-[:alnum:]]*/"
  articulos <- str_extract_all(pagina_web_codigo_penal,patron)
  return(unlist(articulos))
}

```


Una vez en la pagina web del articulo en sí esta funcion se encarga de obtener la estructuracion del articulo, ya sea titulo, libro...
```{r}
#Dada una pagina web de un articulo y la uri de esta devuelve la estructura del articulo
obtener_estructura<-function(URL_articulo,pagina_web_articulo){
  library(stringr)
  
  #Vemos el numero del articulo para ver como recoger la estructura
  numero_articulo <- str_extract(URL_articulo,'[0-9]+')
  
  #Inicializamos las variables para que luego se devuelva el mismo tipo de lista
  titulo_articulo<-"NA"
  libro_articulo<-"NA"
  capitulo_articulo<-"NA"
  seccion_articulo<-"NA"

  #Vemos si el titulo es preliminar
  if (nchar(numero_articulo)<2){
    titulo_articulo<-str_extract(pagina_web_articulo,"TÍTULO PRELIMINAR: [[:alnum:][:punct:] ]*")
  }else{
    #Sacamos el libro
    libro_articulo<-str_extract(pagina_web_articulo,"LIBRO [I]*")
    #Sacamos el titulo
    titulo_articulo<-str_extract(pagina_web_articulo,"T[ií]tulo [IV]*")
    
    #Sacamos el capitulo
    capitulo_articulo<-str_extract(pagina_web_articulo,"Cap[ií]tulo [IVX]*")
    
    #Sacamos la seccion si hay
    seccion_articulo<-str_extract(pagina_web_articulo,"Sección [IVX]*")
    }
    
  lista_resultado<-list(libro=libro_articulo,titulo=titulo_articulo,capitulo=capitulo_articulo,seccion=seccion_articulo)
  return(lista_resultado)
}
  
```


Esta función se usa porque debido a la gran cantidad de paginas web que tenemos que leer he decidido crear una nueva pagina funcion que se encarge de leer las web y obtener la estructura de estas.
Esta se ejecuta hasta que se haya sacado el contenido, continuando su ejecucion incluso si salen errores causados por la lectura de las paginas
```{r}
#Funcion que se encarga de los bucles repetidos
sacar_contenido <- function(inicio_f, final_f, articulos_f, tabla_articulos_f) {
  library(stringr)
  
  # Extraemos los links del rango solicitado
  links_rango <- unlist(articulos_f)[inicio_f:final_f]
  
  #leemos cada uno de los articulos
  for (articulo in links_rango) {
    numero_articulo <- str_extract(articulo, '[0-9]+.*')
    
    #datos iniciales del bucle while    
    pagina_web <- NULL
    intento <- 1
    
    #forzamos a que saque el contenido aunque salgan errores
    while (is.null(pagina_web)) {
      pagina_web <- tryCatch({
        # Pausa pequeña antes de cada lectura (0.5 a 1.5 segundos)
        
        message(paste("Leyendo artículo", numero_articulo, "- Intento:", intento))
        
        # Intentamos leer la página
        paste(readLines(articulo, encoding = "UTF-8", warn = FALSE), collapse = "\n")
        
      }, error = function(e) {
        message(paste("Fallo en el intento", intento, "para el artículo", numero_articulo))
        return(NULL) # Si hay error, devuelve NULL para que el while siga
      })
      
      intento <- intento + 1
    }

    #si se han sacado lo datos guardarlos en el df
    if (!is.null(pagina_web)) {
      lista <- obtener_estructura(articulo, pagina_web)
      
      nueva_fila <- data.frame(
        libro = as.character(lista["libro"]),
        titulo = as.character(lista["titulo"]),
        capitulo = as.character(lista["capitulo"]),
        seccion = as.character(lista["seccion"]),
        articulo = numero_articulo,
        url = articulo,
        stringsAsFactors = FALSE
      )
      
      tabla_articulos_f <- rbind(tabla_articulos_f, nueva_fila)
    
    }
  }
  #retornar el df
  return(tabla_articulos_f)
}
```

Con la funciones de "trabajo" creadas esta ultima funcion se encarga de usarlas para leer la pagina web original, obetener las url y la estructuracion para luego crear un csv donde guarde el contenido
```{r}
#Primera parte del trabajo
primera_parte<-function(){
  
  tabla_articulos <- data.frame(
    libro = character(),
    titulo = character(),
    capitulo = character(),
    seccion = character(),
    articulo = character(),
    url = character(),
    stringsAsFactors = FALSE
  )
  
  
  web<-cargar_texto()
  articulos<-buscar_links_articulos(web)
  
  #Llamamos a la funcion sacar_contenido en varias ocasiones para reducir el posible error causado por tener que leer paginas
  #web
  tabla_articulos <- rbind(tabla_articulos,sacar_contenido(0,50,articulos,tabla_articulos))
  tabla_articulos <- rbind(tabla_articulos,sacar_contenido(50,100,articulos,tabla_articulos))
  tabla_articulos <- rbind(tabla_articulos,sacar_contenido(100,150,articulos,tabla_articulos))
  tabla_articulos <- rbind(tabla_articulos,sacar_contenido(150,200,articulos,tabla_articulos))
  tabla_articulos <- rbind(tabla_articulos,sacar_contenido(200,250,articulos,tabla_articulos))
  tabla_articulos <- rbind(tabla_articulos,sacar_contenido(250,300,articulos,tabla_articulos))
  tabla_articulos <- rbind(tabla_articulos,sacar_contenido(300,350,articulos,tabla_articulos))
  tabla_articulos <- rbind(tabla_articulos,sacar_contenido(350,400,articulos,tabla_articulos))
  tabla_articulos <- rbind(tabla_articulos,sacar_contenido(400,450,articulos,tabla_articulos))
  tabla_articulos <- rbind(tabla_articulos,sacar_contenido(450,500,articulos,tabla_articulos))
  tabla_articulos <- rbind(tabla_articulos,sacar_contenido(500,550,articulos,tabla_articulos))
  tabla_articulos <- rbind(tabla_articulos,sacar_contenido(550,600,articulos,tabla_articulos))
  tabla_articulos <- rbind(tabla_articulos,sacar_contenido(600,650,articulos,tabla_articulos))
  tabla_articulos <- rbind(tabla_articulos,sacar_contenido(650,708,articulos,tabla_articulos))
  
  
  #Guardo los datos en un csv
  write.csv(tabla_articulos, "tabla_articulos.csv", row.names = FALSE)

}
```

Ejeculamos la funcion de la primera parte y vemos que se ejecuta el print. 
```{r}
primera_parte()
print("Funciono")

```


Parte 2 - Web scraping individual de artículos y limpieza del texto


En esta parte se recorre la tabla de URLs (generada en la Parte 1) para visitar cada artículo del Código Penal en la web de Conceptos Jurídicos.  
Para cada artículo se extrae el texto normativo, se limpia eliminando contenido explicativo/no normativo, se insertan saltos de línea para mejorar la legibilidad y se genera:

- 'texto_articulo': texto del artículo limpio y formateado
- 'contexto': libro/título/capítulo/sección concatenados
- 'nombre_doc': nombre de documento plano compatible con quanteda

El resultado final se guarda como `articulos_limpios.rds` y `articulos_limpios.csv`.
En primer lugar se cargan las librerías necesarias y se lee la tabla
de artículos generada en la Parte 1.
```{r, message=FALSE, warning=FALSE}
library(rvest)
library(dplyr)
library(stringr)
library(purrr)

articulos <- read.csv("~/Downloads/tabla_articulos.csv", stringsAsFactors = FALSE)
articulos$articulo <- gsub("/", "", articulos$articulo) #normalizamos a 1 (venía con formato tipo /1)
```

Se define una función que accede a la URL de cada artículo, extrae el texto normativo y elimina contenido explicativo de la web.
```{r}
extraer_texto_articulo <- function(url) {
  pagina <- rvest::read_html(url)
  
  texto <- pagina %>%
    rvest::html_elements("p") %>%
    rvest::html_text() %>%
    paste(collapse = " ") %>%
    stringr::str_squish()
  
  texto <- stringr::str_split(texto, "\\bart\\s*\\d+\\s*cp\\b", n = 2)[[1]][1]
  texto <- stringr::str_split(texto, "\\bEl artículo\\b", n = 2)[[1]][1]
  texto <- stringr::str_squish(texto)
  
  texto
}
```

Para facilitar la lectura del contenido (por ejemplo en inspecciones manuales), se insertan saltos de línea aproximadamente cada 50 caracteres.
```{r}
insertar_saltos <- function(texto, n = 50) {
  stringr::str_wrap(texto, width = n)
}
```


Como quanteda trabaja con documentos “planos”, se genera:

- 'contexto': concatenación de libro, título, capítulo y sección (separados por saltos de línea).
- 'nombre_doc': identificador único en una sola cadena que incluye el número de artículo y su jerarquía.
```{r}
crear_contexto <- function(libro, titulo, capitulo, seccion) {
  partes <- c(libro, titulo, capitulo, seccion)
  partes <- partes[!is.na(partes) & partes != ""]
  paste(partes, collapse = "\n")
}

crear_nombre_doc <- function(libro, titulo, capitulo, seccion, articulo) {
  partes <- c(libro, titulo, capitulo, seccion, paste("Artículo", articulo))
  partes <- partes[!is.na(partes) & partes != ""]
  paste0("art.", articulo, ":", paste(partes, collapse = ":"))
}
```

Antes de lanzar el scraping completo, se realiza una prueba sobre los tres primeros artículos para validar que el texto, el contexto y el nombre del documento se generan correctamente.
```{r}
test <- articulos %>%
  slice(1:3) %>%
  mutate(
    texto_articulo = map_chr(url, extraer_texto_articulo),
    texto_articulo = map_chr(texto_articulo, insertar_saltos),
    contexto = pmap_chr(list(libro, titulo, capitulo, seccion), crear_contexto),
    nombre_doc = pmap_chr(list(libro, titulo, capitulo, seccion, articulo), crear_nombre_doc)
  )

cat(test$nombre_doc[1], "\n\n")
cat(test$contexto[1], "\n\n")
cat(test$texto_articulo[1], "\n")
```

Se aplica el proceso a todos los artículos y se guarda el resultado tanto en formato RDS (para reutilizar en R) como en CSV (para inspección/compartición).
```{r, eval=FALSE}
articulos_limpios <- articulos %>%
  mutate(
    texto_articulo = map_chr(url, extraer_texto_articulo),
    texto_articulo = map_chr(texto_articulo, insertar_saltos),
    contexto = pmap_chr(list(libro, titulo, capitulo, seccion), crear_contexto),
    nombre_doc = pmap_chr(list(libro, titulo, capitulo, seccion, articulo), crear_nombre_doc)
  )

saveRDS(articulos_limpios, "articulos_limpios.rds")
write.csv(articulos_limpios, "articulos_limpios.csv", row.names = FALSE)
```


Parte 3 - Construcción del corpus quanteda y docvars


Leemos el fichero articulos_limpios.csv en un data.frame llamado articulos. Cada fila representa un artículo del Código Penal y contiene, entre otras, las columnas con su texto y su estructura (libro, título, capítulo, sección, nº de artículo, etc.). Usamos stringsAsFactors = FALSE para que las columnas de texto no se conviertan automáticamente en factores (evita problemas al concatenar y limpiar cadenas).
```{r}
articulos <- read.csv("articulos_limpios.csv", stringsAsFactors = FALSE)
```

En quanteda, cada documento del corpus necesita un identificador (docname). Aquí lo construimos con información jerárquica y hacemos limpieza porque en datos reales puede haber campos vacíos:
```{r}
articulos <- articulos %>%
  mutate(
    docname_base = paste(
      libro,
      titulo,
      capitulo,
      seccion,
      paste("Artículo", articulo),
      sep = "."
    ),
    docname_base = str_replace_all(docname_base, "\\bNA\\b\\.?", ""),
    docname_base = str_replace_all(docname_base, "\\.+", "."),
    docname_base = str_replace_all(docname_base, "^\\.|\\.$", ""),
    docname = make.unique(docname_base)
  )
```

Ahora construimos el corpus quanteda
```{r}
corpus_cp <- corpus(
  x = articulos$texto_articulo,
  docnames = articulos$docname
)
```

Añadimos metadatos a cada documento del corpus (como “columnas asociadas” a los documentos). Esto permite luego hacer análisis filtrando o agrupando por estructura del Código Penal
```{r}
docvars(corpus_cp, "libro")    <- articulos$libro
docvars(corpus_cp, "titulo")   <- articulos$titulo
docvars(corpus_cp, "capitulo") <- articulos$capitulo
docvars(corpus_cp, "seccion")  <- articulos$seccion
docvars(corpus_cp, "articulo") <- articulos$articulo
docvars(corpus_cp, "contexto") <- articulos$contexto
```

Comprobamos con summary:
```{r}
summary(corpus_cp)
```


Por último, guardamos el corpus.
```{r}
saveRDS(corpus_cp, "corpus_codigo_penal.rds")

message("OK: corpus_codigo_penal.rds generado correctamente")
```


Reparto de tareas:
- Carlos Herranz Bascuñan: Parte 1 (100%)
- Alberto Barreiro Tasende: Parte 2 (100%)
- Raquel Casinello Montes: Parte 3 (100%)